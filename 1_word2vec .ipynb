{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "word2vec\n",
    "===\n",
    "\n",
    "<img src=\"images/book.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Pop Quiz\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "Do computers prefer numbers or strings?\n",
    "</summary>\n",
    "Numbers. word2vec is currently the best way to convert strings to numbers.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "By The End Of This Session You Should Be Able To:\n",
    "---\n",
    "\n",
    "- Explain why word2vec is powerful and popular\n",
    "- Describe how word2vec is a neural network\n",
    "- Indentify the common architectures of word2vec\n",
    "- Apply word2vec to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Story time...\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = \"\"\"The man and woman meet each other ...\n",
    "         They become king and queen ...\n",
    "         They got old and stop talking to each other. Instead, they read books and magazines ...\n",
    "         \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's hand assign the words to vectors\n",
    "words = ['queen', 'book', 'king', 'magazine', 'woman', 'man']\n",
    "\n",
    "vectors = np.array([[0.1,   0.3],  # queen\n",
    "                    [-0.5, -0.1],  # book\n",
    "                    [0.2,   0.2],  # king\n",
    "                    [-0.3, -0.2],  # magazine\n",
    "                    [-0.5,  0.4],  # car\n",
    "                    [-0.45, 0.3]]) # bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt0lNW9//H3N4AXbBImcIoQSYxBtN6ocgSkUIIuBVFE\npGAAESyHw1EBsbWVSyNJ067qKp62oj3+5KeCP+FgRVpB7RFanFKOWuwBjUe5iZFLErlIQBOrxOT7\n+yPDGCAkgZlcePi81pqV57KfvffsNfOZZ/bMkzF3R0REgimhuTsgIiKNRyEvIhJgCnkRkQBTyIuI\nBJhCXkQkwBTyIiIBFpeQN7NBZrbRzDab2f217O9vZvvNbF3k9pN4tCsiInVrHWsFZpYAPApcAxQD\nb5nZi+6+8Yiiq939pljbExGRhovHmXxPYIu7b3P3CmAxMLSWchaHtkRE5DjEI+RTgR011ndGth3p\nKjN728xeNrOL4tCuiIjUI+bpmgb6HyDN3T83s+uBPwDdmqhtEZFTVjxCvghIq7F+TmRblLuX1Vj+\no5n91sxS3H3fkZWZmf6ZjojIcXL3WqfE4zFd8xbQ1czSzew0IBtYVrOAmXWssdwTsNoCvkZndavn\nNnv27Gbvw8lw0zhprE6FcapLzGfy7l5pZpOBFVS/aDzp7hvMbFL1bn8C+J6Z3QlUAP8Abo21XRER\nqV9c5uTd/b+AC47Y9n9qLD8GPBaPtkREpOF0xetJKisrq7m7cFLQODWcxqphTrZxsvrmc5qamXlL\n65OISEtmZngjfvAqIiItlEJeRCTAFPIiIgGmkBcRCTCFvIhIgCnkRUQCTCEvIhJgCnkRkQBTyIuI\nBJhCXkQkwBTyIiIBppAXEQkwhbyISIAp5EVEAkwhLyISYAr5Bpg/fz433XRTdP3888/n1lu//gXD\ntLQ0CgoKeP311+nZsyehUIhevXrxxhtvRMsMGDCAnJwcvvOd75CYmMjQoUPZt28ft912G8nJyfTq\n1Yvt27dHy0+bNo20tDSSk5O58sorWbNmTXRfXl4et956K+PGjSMpKYlLL72UdevWNfIoiMjJSCHf\nAP3794+GbElJCRUVFdEA//DDDykvL6dLly7ceOONTJs2jU8++YR7772XG264gdLS0mg9zz33HAsX\nLqS4uJgPPviAPn36MGHCBEpLS7nwwgvJy8uLlu3ZsycFBQWUlpYyevRoRowYwcGDB6P7ly9fzujR\nozlw4ABDhgzh7rvvbqLREJGTSVxC3swGmdlGM9tsZvfXUe5KM6sws1vi0W5TycjIIDExkbfffpvV\nq1czcOBAOnfuzObNm1m9ejX9+vXj5Zdfplu3bowePZqEhASys7O58MILWb58ebSeO+64g3PPPZfE\nxESuv/56MjMzGTBgAAkJCYwYMYL169dHy44ePZp27dqRkJDAvffey5dffsmmTZui+/v27cvAgQMx\nM8aOHUtBQUGTjomInBxi/iFvM0sAHgWuAYqBt8zsRXffWEu5B4FXY22zOfTv35/XXnuNDz74gKys\nLEKhEOFwmDfeeIP+/ftTXFxMenr6Ycekp6dTVFQUXe/YsWN0+cwzzzxqvaysLLo+Z84cnnrqKUpK\nSgD47LPP2Lt3b3T/2WefHV1u27YtX3zxBVVVVSQk6M2ZiHwtHonQE9ji7tvcvQJYDAytpdwUYAmw\nOw5tNonCwm3cdlseAwbMZvPm/bzyyiusWbOG/v37893vfpe//OUvrF69mv79+9O5c2c++uijw47f\nvn07qampx93uX//6V375y1+yZMkSSktLKS0tJSkpCf32rYgcr5jP5IFUYEeN9Z1UB3+UmXUGbnb3\nAWZ22L6WqrBwG9deO5etW/OAs4B3MOtJenoqnTt3JjExkbFjx1JZWcnll1/Oueeey9SpU1m8eDEj\nRoxgyZIlbNiwgSFDhhx322VlZbRp04b27dtz8OBBHnzwQT777LM6j9ELgIjUpqne2/8aqDlXX+uv\nirckOTnzawQ8QHfc29OqVTsAEhMTyczMpG/fvpgZKSkpvPTSS8yZM4cOHTowZ84cXn75ZUKhEFD9\na+oNNXDgQAYOHEi3bt3IyMigbdu2dOnSpc5jjqd+ETl1WKxngGbWG8h190GR9emAu/tDNcp8eGgR\n6ACUA//q7stqqc9nz54dXc/KyiIrKyumPp6IAQNmEw7n1bp91aqjt4uINJVwOEw4HI6u5+Xl4e61\nnunFI+RbAZuo/uC1BFgLjHL3Dcco/zSw3N2XHmO/t4Sph9tuy2Phwvv4+kweoJwxY+bw7LOzj3WY\niEiTM7NjhnzM0zXuXglMBlYA7wGL3X2DmU0ys3+t7ZBY22wK+fnjycycTfWbDoByMjNnk58/vtn6\nJCJyvGI+k4+3lnImD9UfvubkzKe4uIrOnRPIzx9PRkZ6vceJiDSlus7kFfIiIie5Rp2uERGRlksh\nLyISYAp5EZEAU8iLiASYQl5EJMAU8iIiAaaQFxEJMIW8iEiAKeRFRAJMIS8iEmAKeRGRAFPIi4gE\nmEJeRCTAFPIiIgGmkBcRCTCFvIhIgCnkm1hGRgZz5syhe/fuJCYmMnHiRHbv3s3gwYNJSkriuuuu\n48CBAwCMHDmSTp06EQqFyMrK4v3334/Wc8cddzB58mRuvPFGkpKSuOqqqygsLGyuuyXSqNavX0+P\nHj1ITk4mOzubUaNGkZOTw4IFC+jXr99hZRMSEvjwww8BOHjwIPfddx/p6el06tSJu+66iy+//DJa\n9qWXXuLyyy8nFArRt29f3n333ei+jIwMHn74Ybp3704oFGLUqFEcPHiwae5wHCnkm8HSpUv585//\nzObNm1m2bBmDBw/mwQcfZO/evVRWVvLII48AMHjwYLZu3cru3bu54oorGDNmzGH1PPfcc+Tl5bF/\n/34yMzOZNWtWc9wdkUZVUVHBsGHDGDduHPv27WPEiBG88MILmFX/ENKhv4fUXL///vv54IMPKCgo\n4IMPPqCoqIif/vSnQPULx4QJE5g3bx779u1j0qRJ3HTTTVRUVESPf/7551mxYgWFhYW88847zJ8/\nv/HvcJzFJeTNbJCZbTSzzWZ2fy37bzKzd8xsvZmtNbPvxKPdk9WUKVPo0KEDnTp1ol+/fvTq1YvL\nLruM0047jWHDhrF+/XoAxo8fT9u2bWnTpg0PPPAA77zzDp999lm0nmHDhtGjRw8SEhIYM2YMb7/9\ndnPdJZFG8+abb/LVV18xdepUWrVqxfDhw7nyyiuPWb7mz4fOmzePX/3qVyQnJ3PWWWcxffp0/vM/\n/zO679/+7d/453/+Z8yMsWPHcvrpp/Pmm29Gj7/nnnvo2LEj7dq1Y8iQISflc6x1rBWYWQLwKHAN\nUAy8ZWYvuvvGGsX+5O7LIuUvBX4HfCvWtk8Wh34QvKioij179lNV9fW+M888k44dOx62XlZWRlVV\nFTNnzmTJkiXs3bsXM8PM2Lt3L4mJiQCcffbZ0ePatm1LWVlZk90nkcZU8zlTUfE+HTp0OGx/enp6\nvXXs2bOHzz//nB49ekS3VVVVRV8Etm3bxjPPPMPcuXOB6heHiooKiouLo+VrPjfbtm1LSUlJTPer\nOcQc8kBPYIu7bwMws8XAUCAa8u7+eY3y3wCqOEUUFm7j2mvnsnVrHnAWsIAf/ej39OnTh4yMYz9Q\nFy1axLJly1i1ahVpaWkcOHCAUCiEfuRcgu7o58yrtGr1PQoLt0WfM9u3b6dr166cddZZfP751/Hy\n8ccfR5c7dOhA27Ztee+99+jUqdNR7XTp0oVZs2YxY8aMxr5LzSoe0zWpwI4a6zsj2w5jZjeb2QZg\nOfD9OLR7UsjJmV/jwQpglJSMIydnfp3HlZWVccYZZxAKhSgvL2fGjBlHzT2KBNHRz5mrqaxMYeTI\nO/nqq69YunQpa9euBaB79+689957FBQU8OWXX5KXl3fYXP3EiROZNm0ae/bsAaCoqIgVK1YAMHHi\nRB5//PFoXeXl5bzyyiuUl5c36f1tbE32wau7/8HdvwXcDPysrrK5ubnRWzgcbpL+NZaioiq+frAC\nGHAmxcXVb2aOFdy33347aWlppKamcskll9CnT59G76tIS3D0c6YN8Hs2blxP+/btef755xk+fDgA\n559/Pjk5OVxzzTV069btqG/aPPTQQ3Tt2pXevXvTrl07rrvuOjZv3gxAjx49mDdvHpMnTyYlJYVu\n3bqxYMGC6LEt+aQqHA4flpN1cveYbkBv4L9qrE8H7q/nmK1AyjH2eZCMGZPrUObgNW5lPmZMbnN3\nTaRFashzZvz48Z6Tk9OMvWxZIrlZa97G40z+LaCrmaWb2WlANrCsZgEzy6yxfAVwmrvvi0PbLV5+\n/ngyM2cDh94ClpOZOZv8/PHN1ieRlkzPmfiK+YNXd680s8nACqqnf5509w1mNql6tz8BDDez24GD\nwD+AkbG2e7LIyEhn5cop5OTMobi4is6dE8jPn1Lnh64ip7KGPGda8lRKS2Pewr6tYWbe0vokItKS\nmRnuXusrn654FREJMIW8iEiAKeRFRAJMIS8iEmAKeRGRAFPIi4gEmEJeRCTAFPIiIgGmkBcRCTCF\nvIhIgCnkRUQCTCEvIhJgCnkRkQBTyIuIBJhCXkTkOGVkZLBq1aqjtl9yySWsXr26GXp0bDH/aIiI\niFT73//93+buwlF0Ji8iEmAKeRGRGGzYsIHzzjuPxYsXHzaNk5eXx6233sq4ceNISkri0ksvZd26\nddHj1q1bxxVXXEFycjIjR44kOzubBx54IO79U8iLiJygdevWMWjQIB577DGys7OP2r98+XJGjx7N\ngQMHGDJkCHfffTcAFRUV3HLLLXz/+99n3759jBo1it///veN0se4hLyZDTKzjWa22czur2X/aDN7\nJ3JbY2aXxqNdEZHmsnr1aoYOHcqzzz7L9ddfX2uZvn37MnDgQMyMsWPHUlBQAMAbb7xBZWUlkydP\nplWrVgwbNoyePXs2Sj9j/uDVzBKAR4FrgGLgLTN70d031ij2IfBddz9gZoOAeUDvWNsWEWkqhYXb\nyMmZT1FRFXv27Oe3v/0tV199Nf369TvmMWeffXZ0uW3btnzxxRdUVVVRUlJCamrqYWW7dOnSKP2O\nx5l8T2CLu29z9wpgMTC0ZgF3f9PdD0RW3wRSERE5SRQWbuPaa+eycOF9hMN5lJcn06pVH7Zs2cIP\nfvCD466vU6dOFBUVHbZtx44d8eruYeIR8qlAzd7tpO4Q/xfgj3FoV0SkSeTkzGfr1jzgrMgW4+OP\nJ9K163WsXr2amTNnNqgedwfgqquuolWrVjz22GNUVlby4osvsnbt2kbpe5N+T97MBgB3AH3rKpeb\nmxtdzsrKIisrq1H7JSJSl6KiKr4OeAADzmTPntNYuXIlV199Na1bt8bM6qzn0P42bdqwdOlSJkyY\nwIwZM7j++usZMmQIp59+eoP6Ew6HCYfDDSprh15ZTpSZ9QZy3X1QZH064O7+0BHlLgNeAAa5+9Y6\n6vNY+yQiEk+33ZbHwoX3cXjQlzNmzByefXZ2XNro3bs3d955J+PGjTvuY80Md6/1FSYe0zVvAV3N\nLN3MTgOygWVHdCCN6oAfW1fAi4i0RPn548nMnA2UR7aUk5k5m/z88Sdc5+rVq9m1axeVlZUsWLCA\nd999l0GDBsXe2SPEPF3j7pVmNhlYQfWLxpPuvsHMJlXv9ieAHCAF+K1Vv1+pcPfG+b6QiEicZWSk\ns3LlFHJy5lBcXEXnzgnk508hIyP9hOvctGkTI0eO5PPPP+e8887jhRdeoGPHjnHsdbWYp2viTdM1\nIiLHp7Gna0REpIVSyIuIBJhCXkQkwBTyIiIBppAXEQkwhbyISIAp5EVEAkwhLyISYAp5EZEAU8iL\niASYQl5EJMAU8iIiAaaQFxEJMIW8iEiAKeRFRAJMIS8iEmAKeRGRAFPIi4gEmEJeRCTA4hLyZjbI\nzDaa2WYzu7+W/ReY2etm9oWZ/SAebYqISP1ax1qBmSUAjwLXAMXAW2b2ortvrFHsE2AKcHOs7YmI\nSMPF40y+J7DF3be5ewWwGBhas4C773X3/wG+ikN7IiLSQPEI+VRgR431nZFtIiLSzGKermkMubm5\n0eWsrCyysrKarS8iIi1NOBwmHA43qKy5e0yNmVlvINfdB0XWpwPu7g/VUnY28Jm7/3sd9XmsfRIR\nOZWYGe5ute2Lx3TNW0BXM0s3s9OAbGBZXf2JQ5siItIAMZ/JQ/VXKIHfUP2i8aS7P2hmk6g+o3/C\nzDoCfwcSgSqgDLjI3ctqqUtn8iIix6GuM/m4hHw8KeRFRI5PY0/XiIhIC6WQFxEJMIW8iEiAKeRF\nRAJMIS8iEmAKeRGRAFPIi4gEmEJeRCTAFPIiIgGmkBcRCTCFvIhIgCnkRUQCTCEvIhJgCnkRkQBT\nyIuIBJhCXkQkwBTyIiIBppAXEQkwhbyISIDFJeTNbJCZbTSzzWZ2/zHKPGJmW8zsbTP7djzaFRGR\nusUc8maWADwKDAQuBkaZ2YVHlLkeyHT384FJwOOxtisiIvWLx5l8T2CLu29z9wpgMTD0iDJDgWcA\n3P1vQLKZdYxD2yIiUod4hHwqsKPG+s7ItrrKFNVSRkRE4qx1c3egNrm5udHlrKwssrKymq0vIiIt\nTTgcJhwON6isuXtMjZlZbyDX3QdF1qcD7u4P1SjzOPCauz8XWd8I9Hf3XbXU57H2SUTkVGJmuLvV\nti8e0zVvAV3NLN3MTgOygWVHlFkG3B7pTG9gf20BLyIi8RXzdI27V5rZZGAF1S8aT7r7BjObVL3b\nn3D3V8xssJl9AJQDd8TaroiI1C/m6Zp403SNiMjxaezpGhERaaEU8iIiAaaQFxEJMIW8iEiAKeRF\nRAJMIS8iEmAKeRGRAFPIi4gEmEJeRCTAFPIiIgGmkBcRCTCFvIhIgCnkRUQCTCEvIhJgCvkGysjI\nYNWqVXGtc8CAATz11FNxrVNEpCaFvIhIgCnkRUQCTCF/HNauXcvFF19M+/btmTBhAgcPHgRg3rx5\nnH/++XTo0IGbb76ZkpKS6DGvv/46PXv2JBQK0atXL954441a6y4pKaF79+48/PDDTXJfROTUoJA/\nDosWLWLlypVs3bqVTZs28bOf/YzXXnuNmTNnsmTJEkpKSkhLSyM7OxuA0tJSbrzxRqZNm8Ynn3zC\nvffeyw033EBpaelh9X700UdkZWUxdepUfvjDHzbHXRORgIop5M0sZGYrzGyTmb1qZsnHKPekme0y\ns4JY2mtuU6ZMoXPnzrRr145Zs2axaNEiFi5cyIQJE+jevTtt2rThF7/4BW+++Sbbt2/n5Zdfplu3\nbowePZqEhASys7O58MILWb58ebTO9957jwEDBpCfn8+ECROa8d6JSBDFeiY/HfiTu18ArAJmHKPc\n08DAGNtqcoWF27jttjwGDJjNnj37ad26TXRfeno6xcXFlJSUkJ6eHt1+1llnkZKSQlFREcXFxYft\nO3RcUVFRdH3RokWcc845DB8+vPHvkIiccmIN+aHAgsjyAuDm2gq5+xqgtLZ9LVVh4TauvXYuCxfe\nRzicR3l5Mvffv4jCwm0AbN++ndTUVDp37sxHH30UPa68vJxPPvmk1n01jzskNzeXDh06MGrUKNy9\nKe6aiJxCYg35b7r7LgB3/xj4ZuxdahlycuazdWsecFZki7FnTwk//OEj7Nu3j5///OdkZ2eTnZ3N\n/PnzKSgo4Msvv2TmzJn07t2btLQ0Bg8ezJYtW1i8eDGVlZU899xzbNiwgSFDhkTbadOmDc8//zzl\n5eWMHTtWQS8icdW6vgJmthLoWHMT4MBPaikel4TKzc2NLmdlZZGVlRWPao9LUVEVXwc8VN/t23j1\n1Yfp2vVpbr75ZmbNmsUZZ5xBfn4+t9xyC/v376dPnz4sXrwYgJSUFF566SWmTp3KnXfeSdeuXXn5\n5ZcJhULVNZoB0Lp1a5YuXcqQIUOYMGGCLpASkTqFw2HC4XCDylosZ45mtgHIcvddZnY28Jq7f+sY\nZdOB5e5+WT11eks4m73ttjwWLryPw4O+nDFj5vDss7Obq1siIkcxM9zdatsX63TNMmB8ZHkc8GJd\n/YjcTgr5+ePJzJwNlEe2lJOZOZv8/PHN1icRkeMV65l8CvA7oAuwDRjp7vvNrBMwz91vjJRbBGQB\n7YFdwGx3f/oYdbaIM3mo/vA1J2c+xcVVdO6cQH7+eDIy0us9TkSkKdV1Jh9TyDeGlhTyIiIng8ac\nrhERkRZMIS8iEmAKeRGRAFPIi4gEmEJeRCTAFPIiIgGmkBcRCTCFvIhIgCnkRUQCTCEvIhJgCnkR\nkQBTyIuIBJhCXkQkwBTycspYs2YN3/pWrb9pIxJY+lfDIiInOf2rYRGRU5RCXuIqIyODOXPm0L17\ndxITE5k4cSK7d+9m8ODBJCUlcd1113HgwAEARo4cSadOnQiFQmRlZfH+++9H69m3bx9DhgwhOTmZ\nXr16kZOTQ79+/aL7p02bRlpaGsnJyVx55ZWsWbMmui8UCpGUlERSUhLf+MY3SEhIYPv27fzlL3+h\nS5cuh/X14Ycfpnv37oRCIUaNGsXBgwej+1966SUuv/xyQqEQffv25d13323MoRNpFAp5ibulS5fy\n5z//mc2bN7Ns2TIGDx7Mgw8+yN69e6msrOSRRx4BYPDgwWzdupXdu3dzxRVXMGbMmGgdd911F4mJ\niezevZv58+ezYMECzL5+N9qzZ08KCgooLS1l9OjRjBgxIhrQpaWlfPrpp3z66afcc8899O/fn9TU\nVIDD6gB4/vnnWbFiBYWFhbzzzjvMnz8fgPXr1zNhwgTmzZvHvn37mDRpEjfddBMVFRWNOXQi8efu\nJ3wDQsAKYBPwKpBcS5lzgFXAe8C7wNR66nQ5eZ177rm+aNGi6Prw4cP9rrvuiq7PnTvXhw0bdtRx\npaWlbmb+6aefemVlpbdp08a3bNkS3f+Tn/zE+/Xrd8x2Q6GQFxQUHLZt8eLFnpGR4Z988om7u4fD\nYe/Spcsx+/rjH//Y77zzTnd3v/POO/2BBx44rL4LLrjAV69eXef9F2kOkdysNVNjPZOfDvzJ3S+I\nBPmMWsp8BfzA3S8GrgLuNrMLY2xXWrCOHTtGl88888yj1svKyqiqqmL69Ol07dqVdu3akZGRgZmx\nd+9e9uzZQ2VlJeecc070uJrTLABz5szhoosuIhQKEQqF+PTTT9m7d290//r165kyZQp/+MMfSElJ\naVBf27ZtS1lZGQDbtm3j4YcfJiUlhZSUFEKhEDt37qS4uPjEB0akGbSO8fihQP/I8gIgTHXwR7n7\nx8DHkeUyM9sApAIbY2xbWojCwm3k5MynqKiKPXv2U1Kyq95jFi1axLJly1i1ahVpaWkcOHCAUCiE\nu/NP//RPtG7dmp07d9K1a1cAduzYET32r3/9K7/85S957bXXuOiiiwBISUk59E6Q3bt3M2zYMP7j\nP/6Dyy677ITuU5cuXZg1axYzZtR23iJy8oj1TP6b7r4LomH+zboKm9m5wLeBv8XYrrQQhYXbuPba\nuSxceB/hcB7l5cn86Ee/p7BwW53HlZWVccYZZxAKhSgvL2fGjBnR+fKEhARuueUWcnNz+cc//sHG\njRt55plnDju2TZs2tG/fnoMHD/LTn/6Uzz77DIDKykq+973vMXbsWIYPH37C92vixIk8/vjjrF27\nFoDy8nJeeeUVysvLT7hOkeZQb8ib2UozK6hxezfy96Zaih/zC+5m9g1gCXCPu5fF0GdpQXJy5rN1\nax5wVmRLAiUl48jJmQ8c/UHnIbfffjtpaWmkpqZyySWX0KdPn8P2z507l/3799OpUyfGjRvH6NGj\nOf300wEYOHAgAwcOpFu3bmRkZNC2bdvodM7OnTv57//+b37961+TlJREYmIiSUlJ7Ny586g+HKtv\nAD169GDevHlMnjyZlJQUunXrxoIFC45vcERagJguhopMvWS5+y4zOxt4zd2PuqTQzFoDLwF/dPff\n1FOnz549O7qelZVFVlbWCfdRGteAAbMJh/Nq3b5q1dHbT9T06dPZtWsXTz/9dNzqFDlZhcNhwuFw\ndD0vL++YF0PFOie/DBgPPASMA148RrmngPfrC/hDcnNzY+yWNJXU1ASgnK/P5AHK6dw5tpnATZs2\ncfDgQS699FLWrl3Lk08+yVNPPRVTnSJBceTJb17esU+oYj2TTwF+B3QBtgEj3X2/mXUC5rn7jWb2\nHWA11V+f9Mhtprv/1zHq9Fj6JE3r0Jz811M25WRmzmblyilkZKSfcL1///vfGTVqFCUlJXTs2JFJ\nkybx4x//OG79FgmSuv6tgf53jcTs0Ldriour6Nw5gfz88TEFvIgcH4W8iEiA6R+UiYicohTyIiIB\nppAXEQkwhbyISIAp5EVEAkwhLyISYAp5EZEAU8iLiASYQl5EJMAU8iIiAaaQFxEJMIW8iEiAKeRF\nRAJMIS8iEmAKeRGRAFPIi4gEmEJeRCTAFPIiIgEWU8ibWcjMVpjZJjN71cySaylzupn9zczWm9m7\nZjY7ljZFRKThYj2Tnw78yd0vAFYBM44s4O5fAgPc/XLg28D1ZtYzxnZPeeFwuLm7cFLQODWcxqph\nTrZxijXkhwILIssLgJtrK+Tun0cWTwdaA/ql7hidbA+05qJxajiNVcOcbOMUa8h/0913Abj7x8A3\naytkZglmth74GFjp7m/F2K6IiDRA6/oKmNlKoGPNTVSfif+kluK1nqG7exVwuZklAX8ws4vc/f0T\n6K+IiBwHcz/xmRMz2wBkufsuMzsbeM3dv1XPMTlAubv/+zH2aypHROQ4ubvVtr3eM/l6LAPGAw8B\n44AXjyxgZh2ACnc/YGZnAtcCDx5vR0VE5PjFeiafAvwO6AJsA0a6+34z6wTMc/cbzexSqj+UTYjc\nnnP3n8fedRERqU9MIS8iIi1bs17x2pCLqSLlks3seTPbYGbvmVmvpu5rc2voWEXKJpjZOjNb1pR9\nbAkaeIHeOWa2KvJYetfMpjZHX5uDmQ0ys41mttnM7j9GmUfMbIuZvW1m327qPrYU9Y2VmY02s3ci\ntzWRWYvQm55PAAAC2UlEQVQWp7n/rUG9F1NF/AZ4JfKhbndgQxP1ryVp6FgB3AOcqt9easg4fQX8\nwN0vBq4C7jazC5uwj83CzBKAR4GBwMXAqCPvt5ldD2S6+/nAJODxJu9oC9CQsQI+BL7r7t2BnwHz\nmraXDdPcIV/vxVSRr132c/enAdz9K3f/tOm62GI06MIzMzsHGAz83ybqV0tT7zi5+8fu/nZkuYzq\nk4bUJuth8+kJbHH3be5eASymerxqGgo8A+DufwOSzawjp556x8rd33T3A5HVN2mhj6HmDvmGXEyV\nAew1s6cjUxBPRL6lc6pp0IVnwK+AH3HqXlXc0HECwMzOpfrfbfyt0XvW/FKBHTXWd3J0MB1ZpqiW\nMqeChoxVTf8C/LFRe3SCYv0KZb3icDFVa+AK4G53/7uZ/Zrqt+SB+0dnsY6Vmd0A7HL3t80sK3J8\n4MTjAr1IPd8AlgD3RM7oRY6bmQ0A7gD6NndfatPoIe/u1x5rn5ntMrOONS6m2l1LsZ3ADnf/e2R9\nCVDrB0YnuziM1XeAm8xsMHAmkGhmz7j77Y3U5WYRh3HCzFpT/Vj6f+5+1PUdAVUEpNVYPyey7cgy\nXeopcypoyFhhZpcBTwCD3L20ifp2XJp7uubQxVRwjIupIm+9d5hZt8imazg1P1RsyFjNdPc0dz8P\nyAZWBS3gG6DecYp4Cnjf3X/TFJ1qId4CuppZupmdRvVj5MhvYC0Dbgcws97A/kPTX6eYesfKzNKA\nF4Cx7r61GfrYMO7ebDcgBfgTsAlYAbSLbO8EvFSjXHeqB/1tYCmQ3Jz9bsljVaN8f2BZc/e7JY4T\n1e94KiOPp/XAOqrPxJq9/00wPoMiY7MFmB7ZNgn41xplHgU+AN4BrmjuPrfUsaL62zSfRB4/64G1\nzd3n2m66GEpEJMCae7pGREQakUJeRCTAFPIiIgGmkBcRCTCFvIhIgCnkRUQCTCEvIhJgCnkRkQD7\n/5Y+4aqT8CSFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110392828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(vectors[:,0], vectors[:,1], 'o')\n",
    "plt.xlim(-0.6, 0.3)\n",
    "plt.ylim(-0.3, 0.5)\n",
    "for word, x, y in zip(words, vectors[:,0], vectors[:,1]):\n",
    "    plt.annotate(word, (x, y), size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "How many dimensions are data represented in? How many dimensions would we need to represent for typical word vectors?\n",
    "</summary>\n",
    "There are 2 dimensions. Typicall word vectors would need 5 (n-1). You would could a baseline words as all zeros.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "word2vec creates dense representations\n",
    "----\n",
    "\n",
    "![](images/dense_repsentations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Distributional Hypothesis\n",
    "---\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/John_Rupert_Firth.png/220px-John_Rupert_Firth.png)\n",
    "\n",
    ">“You shall know a word\n",
    ">by the company it keeps”\n",
    "\n",
    "> \\- J. R. Firth 1957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Example\n",
    "---\n",
    "\n",
    "> ... government debt problems are turning into __banking__ crises...  \n",
    "\n",
    "> ... Europe governments needs unified __banking__ regulation to replace the hodgepodge of debt regulations...\n",
    "\n",
    "These words (e.g., government, regulation) will represent __banking__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "How does word2vec model the Distributional Hypothesis?\n",
    "---\n",
    "\n",
    "Input = text corpus  \n",
    "Output = vector for each word\n",
    "\n",
    "Word2Vec is a very simple neural network with a single hidden layer. \n",
    "![](images/w2v_neural_net.png)\n",
    "\n",
    "Note: The bow-tie shape. That is is an __autoencoder__. Autoencoders compress sparse representations into dense representation. The neural network learns the mapping that best preserves the structure of the orginal space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Neural Net Sidebar\n",
    "----\n",
    "\n",
    "![](https://askabiologist.asu.edu/sites/default/files/resources/articles/neuron_anatomy.jpg)\n",
    "\n",
    "### By The End of This Sidebar You should be able to:\n",
    "1. Identify the components of a neural net.\n",
    "2. Explain how neural represent data.\n",
    "3. Understand how backprogragation algorithm enables the updates of wieghts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demystifying neural networks\n",
    "  \n",
    "![](images/single_neuron.png)  \n",
    "\n",
    "Neural nets take many inputs, aggregate the singals, and produces an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[See Appendix of _word2vec Parameter Learning Explained_ for complete review of neural nets](http://arxiv.org/abs/1411.2738)\n",
    "\n",
    "![](images/neuron_math.png)\n",
    "\n",
    "y = f(u)\n",
    "\n",
    "where u is a scalar number, which is the net input (or “new input”) of the neuron. u is\n",
    "defined as\n",
    "\n",
    "![](images/input.png)\n",
    "x is a vector of inputs  \n",
    "w is a vector of weights  \n",
    "\n",
    "**Note:** Ingoring bias term, add an input add an input dimension (e.g., x0) that is constant 1.\n",
    "u = b + sum(x *w)\n",
    "where b is basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Logistic function\n",
    "----\n",
    "\n",
    "f(u) where f is activation function/decision/transfer function.  \n",
    "f(u) is the logistic function, – the most common sigmoid non-linearity. (it should look famailar)\n",
    "\n",
    "![](images/logistic_function.png)\n",
    "\n",
    "![](images/logistic_function_graph.png)\n",
    "\n",
    "Neural Nets are like logistic regression.\n",
    "\n",
    "Good properties:\n",
    "- the output y is always between 0 and 1, \n",
    "- σ(u) is smooth and differentiable, making the derivation of update equation very easy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Updating the weights\n",
    "----\n",
    "\n",
    "![](images/basic_neural_net.png)\n",
    "\n",
    "1. Intial random weights.  \n",
    "2. Calculate loss function.  \n",
    "3. Update weights via back propgation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "What is a loss function? Give a couple of examples.\n",
    "</summary>\n",
    "A lost function is how you weigh your errors.\n",
    "\n",
    "For example, sum of squared residuals heavily penalizes large misses. While hinge loss ignores some errors all together.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Gradient descent: The secret sauce of neural networks\n",
    "---\n",
    "\n",
    "Define the error function, i.e., the training objective.\n",
    "\n",
    "Objective function:\n",
    "![](images/objective_function.png)\n",
    "\n",
    "\n",
    "Apply gradient descent:\n",
    "![](images/stochastic_gradient_descent.png)\n",
    "\n",
    "NOTE: This is technically stochastic gradient descent. It is a minor variation that is much more pratical and used more often in pratice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "How does this look during training?\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://player.vimeo.com/video/112168934\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.VimeoVideo at 0x110377a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, VimeoVideo\n",
    "\n",
    "display(VimeoVideo(112168934))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The 2 architectures of word2vec\n",
    "----\n",
    "\n",
    "1. “Continuous bag of words”: Predict a missing word in a sentence based on the surrounding context\n",
    "\n",
    "2. “Skip-gram”: Each current word as an input to a log-linear classifier to predict words within a certain range before and after that current word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous bag of words (CBOW) architecture\n",
    "\n",
    "<img src=\"images/cbow.png\" style=\"width: 400px;\"/>\n",
    "Given the context (surronding words), predict the current word.\n",
    "\n",
    "[Detailed explanation](http://alexminnaar.com/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Skip-gram architecture (the inverse of CBOW)\n",
    "\n",
    "<img src=\"images/skip-gram.png\" style=\"width: 400px;\"/>\n",
    "Given the current word, predict the context (surrounding words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Defining skip-grams\n",
    "---\n",
    "\n",
    "![](images/skip-gram-equation.png)\n",
    "\n",
    "k-skip-n-grams for a sentence w1... wm to be the set\n",
    "\n",
    "Skip-grams reported for a certain skip distance k allow a total of k or less skips to construct the n-gram. As such, “4-skip-n-gram” results include 4 skips, 3 skips, 2 skips, 1 skip, and 0 skips (typical n-grams formed from adjacent words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Skip-gram example\n",
    "---\n",
    "\n",
    "Here is an actual sentence example showing 2-skip-bi-grams and tri-grams compared to standard bi-grams and trigrams consisting of adjacent words for the sentence:\n",
    "\n",
    ">“Insurgents killed in ongoing fighting.”\n",
    "\n",
    "Bi-grams = {insurgents killed, killed in, in ongoing, ongoing fighting}.  \n",
    "\n",
    "2-skip-bi-grams = {insurgents killed, insurgents in, insurgents ongoing, killed in, killed ongoing, killed fighting, in ongoing, in fighting, ongoing fighting}.  \n",
    "\n",
    "Tri-grams = {insurgents killed in, killed in ongoing, in ongoing fighting}.  \n",
    "\n",
    "2-skip-tri-grams = {insurgents killed in, insurgents killed ongoing, insurgents killed fighting, insurgents in ongoing, insurgents in fighting, insurgents ongoing fighting, killed in ongoing, killed in fighting, killed ongoing fighting, in ongoing fighting}. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram architecture, deep dive\n",
    "\n",
    "![](images/skip_gram_detailed.png)\n",
    "\n",
    "The target word is now at the input layer, and the context words are on the output layer.\n",
    "\n",
    "On the output layer, instead of outputing one multinomial distribution, we are outputing C multinomial distributions. Each output is computed using the same hidden to output matrix\n",
    "\n",
    "Objective function:\n",
    "![](images/multinomial_distributions.png)\n",
    "\n",
    "Because the output layer panels share the same weights,\n",
    "\n",
    "Loss function:\n",
    "![](images/skip_gram_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW vs. Skip-gram\n",
    "CBOW is several times faster to train than the skip-gram and has slightly better accuracy for the frequent words.  \n",
    "\n",
    "Skip-gram works well with small amount of the training data and well represents rare words.\n",
    "\n",
    "Skip-gram tends to be the most commmon architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Now that we have word vectors, what can we do? Math with words!\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Distance\n",
    "---\n",
    "\n",
    "![](http://blog.krecan.net/wp-content/family.png)\n",
    "\n",
    "The word vectors are directions in space and can encode relationships between words.  \n",
    "\n",
    "The proximity of words to each other can be calculated through their cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cosine_sim](https://upload.wikimedia.org/math/f/3/6/f369863aa2814d6e283f859986a1574d.png)\n",
    "\n",
    "1 meaning exactly the same  \n",
    "0 indicating orthogonality (decorrelation)  \n",
    "−1 meaning exactly opposite  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests pass :)\n"
     ]
    }
   ],
   "source": [
    "def cos_sim(v1, v2):\n",
    "   \"Calculate cosine similarity between vector 1 and 2\"\n",
    "   return v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_cos_sim():\n",
    "    v1 = np.array([1, 2, 3])\n",
    "    v2 = np.array([-1, -2, -3])\n",
    "    v3 = np.array([0, 3])\n",
    "    v4 = np.array([4, 0])\n",
    "    v5 = np.array([3, 45, 7, 2])\n",
    "    v6 = np.array([2, 54, 13, 15])\n",
    "    assert cos_sim(v1, v1) == 1.0\n",
    "    assert cos_sim(v1, v2) == -1.0\n",
    "    assert cos_sim(v3, v4) == 0.0\n",
    "    assert round(cos_sim(v5, v6), 4) == round(0.97228425171235, 4)\n",
    "    return \"tests pass :)\"\n",
    "    \n",
    "print(test_cos_sim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words closest to “Sweden”\n",
    "![](http://deeplearning4j.org/img/sweden_cosine_distance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Extention to analogies, aka linear relationships\n",
    "---\n",
    "\n",
    "### Male-Female\n",
    "![](http://multithreaded.stitchfix.com/assets/images/blog/vectors.gif)\n",
    "\n",
    "[Demo](http://rare-technologies.com/word2vec-tutorial/#app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plurals\n",
    "\n",
    "![](images/plurals.png)  \n",
    "\n",
    "Different paths through word2vec space encode different relationships. More on this next time with doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verb Tense\n",
    "\n",
    "![](images/verb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country-Captial\n",
    "![](images/country.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Use word2vec to build data products\n",
    "----\n",
    "\n",
    "When I worked at an employment website, I built a recommendation engine for job seekers.  \n",
    "<br>\n",
    "<details><summary>\n",
    "What improved job would you recommend to a Babysitter?\n",
    "</summary>\n",
    "A Nanny. \n",
    "<br>\n",
    "A Nanny is a Babysitter as Senior Engineer is to a Engineer.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Word2vec implemenation\n",
    "---\n",
    "\n",
    "### Code\n",
    "[Google’s word2vec](https://code.google.com/p/word2vec/)  \n",
    "\n",
    "__Hightlights__:\n",
    "\n",
    "- Written in C\n",
    "- Highly optimitized\n",
    "- Light on documentation and transparency \n",
    "\n",
    "[Python’s Gensim package](https://radimrehurek.com/gensim/)  \n",
    "[Google’s TensorFlow](https://www.tensorflow.org/versions/r0.8/tutorials/word2vec/index.html)\n",
    "\n",
    "### Corpus\n",
    "> \"Data is the world's best regularizer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "How do we evaluate word2vec, especially if it is built on a custom corpus?\n",
    "</summary>\n",
    "Word2Vec is an unsupervised learning algorithm. Thus there’s no good way to objectively evaluate the result. \n",
    "\n",
    "One possible method is to compare analogies performance with pretrained Google vectors.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Summary\n",
    "---\n",
    "\n",
    "- Word2Vec is popular because it is easy to implement and creates a useful abstraction of embeddeding words in a high (but tractable) dimensional space.\n",
    "- Word2Vec is a _relatively_ simple neural net with 1 input layer, 1 hidden layer, and 1 output layer.\n",
    "- There are 2 common ways to represent context: \n",
    "    1. CBOW: given context, predict word\n",
    "    2. skip-gram: given word, predict context\n",
    "- Once trained, any vector operations can be applied to words. The most common operations are: arithmetic, distance, and clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bonus Material\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Speeding up skip-gram\n",
    "---\n",
    "\n",
    "Since Skip-gram is slow (look at the architecture), the smart people at The Google optimitizated within the architecture.\n",
    "\n",
    "> When in doubt, throw a binary tree at it.\n",
    "\n",
    "This particular binary tree is call _Hierarchical Softmax_. \n",
    "\n",
    "### What the hell is softmax?\n",
    "\n",
    "It is a normalized exponential.\n",
    "\n",
    "![](images/softmax_function.png)\n",
    "\n",
    "J is the current class. \n",
    "K is all classes.\n",
    "\n",
    "Generalization of the logistic function to multi-class.\n",
    "\n",
    "Used in various probabilistic multiclass classification methods:\n",
    "\n",
    "- multinomial logistic regression\n",
    "- multiclass linear discriminant analysis\n",
    "- naive Bayes classifiers \n",
    "- artificial neural networks\n",
    "\n",
    "[Source](https://en.wikipedia.org/wiki/Softmax_function)\n",
    "\n",
    "### Okay then ... What is Hierarchical Softmax?\n",
    "\n",
    "![](images/binary_tree.png)\n",
    "\n",
    "Uses a binary tree as a data structure to represent all words in the vocabulary. The V words must be leaf nodes of the tree. For each leaf node, there exists an unique path from the root to the node. This path is used to estimate the probability of the word represented by the leaf node.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
